<html><body><div>
<main><div>
<div><nav><ul>
<li>
<a title="GitHub Copilot" href="/en/copilot">GitHub Copilot</a><span>/</span>
</li>
<li>
<a title="Use GitHub Copilot" href="/en/copilot/using-github-copilot">Use GitHub Copilot</a><span>/</span>
</li>
<li>
<a title="AI models" href="/en/copilot/using-github-copilot/ai-models">AI models</a><span>/</span>
</li>
<li><a title="Choose the right AI model" href="/en/copilot/using-github-copilot/ai-models/choosing-the-right-ai-model-for-your-task">Choose the right AI model</a></li>
</ul></nav></div>
<div>
<div><div><h1>Choosing the right AI model for your task</h1></div></div>
<div><div><p>Compare available AI models in Copilot Chat and choose the best model for your task.</p></div></div>
<div>
<h2>In this article</h2>
<nav><ul>
<li><a href="#comparison-of-ai-models-for-github-copilot"><div><span>Comparison of AI models for GitHub Copilot</span></div></a></li>
<li><a href="#gpt-41"><div><span>GPT-4.1</span></div></a></li>
<li><a href="#gpt-4o"><div><span>GPT-4o</span></div></a></li>
<li><a href="#gpt-45"><div><span>GPT-4.5</span></div></a></li>
<li><a href="#o1"><div><span>o1</span></div></a></li>
<li><a href="#o3"><div><span>o3</span></div></a></li>
<li><a href="#o3-mini"><div><span>o3-mini</span></div></a></li>
<li><a href="#o4-mini"><div><span>o4-mini</span></div></a></li>
<li><a href="#claude-35-sonnet"><div><span>Claude 3.5 Sonnet</span></div></a></li>
<li><a href="#claude-37-sonnet"><div><span>Claude 3.7 Sonnet</span></div></a></li>
<li><a href="#claude-sonnet-4"><div><span>Claude Sonnet 4</span></div></a></li>
<li><a href="#claude-opus-4"><div><span>Claude Opus 4</span></div></a></li>
<li><a href="#gemini-20-flash"><div><span>Gemini 2.0 Flash</span></div></a></li>
<li><a href="#gemini-25-pro"><div><span>Gemini 2.5 Pro</span></div></a></li>
<li><a href="#further-reading"><div><span>Further reading</span></div></a></li>
</ul></nav>
</div>
<div><div><div>
<h2><a href="#comparison-of-ai-models-for-github-copilot">Comparison of AI models for GitHub Copilot</a></h2>
<p>GitHub Copilot supports multiple AI models with different capabilities. The model you choose affects the quality and relevance of responses by Copilot Chat and Copilot code completion. Some models offer lower latency, while others offer fewer hallucinations or better performance on specific tasks.</p>
<p>This article helps you compare the available models, understand the strengths of each model, and choose the model that best fits your task. For guidance across different models using real-world tasks, see <a href="/en/copilot/using-github-copilot/ai-models/comparing-ai-models-using-different-tasks">Comparing AI models using different tasks</a>.</p>
<p>The best model depends on your use case:</p>
<ul>
<li>For <strong>balance between cost and performance</strong>, try GPT-4.1 or Claude 3.7 Sonnet.</li>
<li>For <strong>fast, low-cost support for basic tasks</strong>, try o4-mini or Claude 3.5 Sonnet.</li>
<li>For <strong>deep reasoning or complex coding challenges</strong>, try o3, GPT-4.5, or Claude 3.7 Sonnet.</li>
<li>For <strong>multimodal inputs and real-time performance</strong>, try Gemini 2.0 Flash or GPT-4.1.</li>
</ul>
<p>You can click a model name in the list below to jump to a detailed overview of its strengths and use cases.</p>
<ul>
<li><a href="#gpt-41">GPT-4.1</a></li>
<li><a href="#gpt-4o">GPT-4o</a></li>
<li><a href="#gpt-45">GPT-4.5</a></li>
<li><a href="#o1">o1</a></li>
<li><a href="#o3">o3</a></li>
<li><a href="#o3-mini">o3-mini</a></li>
<li><a href="#o4-mini">o4-mini</a></li>
<li><a href="#claude-35-sonnet">Claude 3.5 Sonnet</a></li>
<li><a href="#claude-37-sonnet">Claude 3.7 Sonnet</a></li>
<li><a href="#claude-sonnet-4">Claude Sonnet 4</a></li>
<li><a href="#claude-opus-4">Claude Opus 4</a></li>
<li><a href="#gemini-20-flash">Gemini 2.0 Flash</a></li>
<li><a href="#gemini-25-pro">Gemini 2.5 Pro</a></li>
</ul>
<div>
<p> Different models have different premium request multipliers, which can affect how much of your monthly usage allowance is consumed. For details, see <a href="/en/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests">About premium requests</a>.</p>
</div>
<h2><a href="#gpt-41">GPT-4.1</a></h2>
<p>OpenAI’s latest model, GPT-4.1, is now available in GitHub Copilot and GitHub Models, bringing OpenAI’s newest model to your coding workflow. This model outperforms GPT-4o across the board, with major gains in coding, instruction following, and long-context understanding. It has a larger context window and features a refreshed knowledge cutoff of June 2024.</p>
<p>OpenAI has optimized GPT-4.1 for real-world use based on direct developer feedback about: frontend coding, making fewer extraneous edits, following formats reliably, adhering to response structure and ordering, consistent tool usage, and more. This model is a strong default choice for common development tasks that benefit from speed, responsiveness, and general-purpose reasoning.</p>
<h3><a href="#use-cases">Use cases</a></h3>
<p>GPT-4.1 is a revamped version of OpenAI's GPT-4o model. This model is a strong default choice for common development tasks that benefit from speed, responsiveness, and general-purpose reasoning. If you're working on tasks that require broad knowledge, fast iteration, or basic code understanding, GPT-4.1 makes large improvements over GPT-4o.</p>
<h3><a href="#strengths">Strengths</a></h3>
<p>The following table summarizes the strengths of GPT-4.1:</p>
<div>



































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why GPT-4.1 is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code explanation</th>
<td>Understand what a block of code does or walk through logic.</td>
<td>Fast and accurate explanations.</td>
</tr>
<tr>
<th>Code commenting and documentation</th>
<td>Generate or refine comments and documentation.</td>
<td>Writes clear, concise explanations.</td>
</tr>
<tr>
<th>Bug investigation</th>
<td>Get a quick explanation or suggestion for an error.</td>
<td>Provides fast diagnostic insight.</td>
</tr>
<tr>
<th>Code snippet generation</th>
<td>Generate small, reusable pieces of code.</td>
<td>Delivers high-quality results quickly.</td>
</tr>
<tr>
<th>Multilingual prompts</th>
<td>Work with non-English prompts or identifiers.</td>
<td>Improved multilingual comprehension.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options">Alternative options</a></h3>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<td>Multi-step reasoning or algorithms</td>
<td>Design complex logic or break down multi-step problems.</td>
<td>GPT-4.5 or Claude 3.7 Sonnet provide better step-by-step thinking.</td>
</tr>
<tr>
<td>Complex refactoring</td>
<td>Refactor large codebases or update multiple interdependent files.</td>
<td>GPT-4.5 handles context and code dependencies more robustly.</td>
</tr>
<tr>
<td>System review or architecture</td>
<td>Analyze structure, patterns, or architectural decisions in depth.</td>
<td>Claude 3.7 Sonnet or GPT-4.5 offer deeper analysis.</td>
</tr>
</tbody>
</table>
<h2><a href="#gpt-4o">GPT-4o</a></h2>
<p>OpenAI GPT-4o is a multimodal model that supports text and images. It responds in real time and works well for lightweight development tasks and conversational prompts in Copilot Chat.</p>
<p>Compared to previous models, GPT-4o improves performance in multilingual contexts and demonstrates stronger capabilities when interpreting visual content. It delivers GPT-4 Turbo–level performance with lower latency and cost, making it a good default choice for many common developer tasks.</p>
<p>For more information about GPT-4o, see <a href="https://platform.openai.com/docs/models/gpt-4o">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-1">Use cases</a></h3>
<p>GPT-4o is a good choice for common development tasks that benefit from speed, responsiveness, and general-purpose reasoning. If you're working on tasks that require broad knowledge, fast iteration, or basic code understanding, GPT-4o is likely the model to use.</p>
<h3><a href="#strengths-1">Strengths</a></h3>
<p>The following table summarizes the strengths of GPT-4o:</p>
<div>








































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why GPT-4o is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code explanation</th>
<td>Understand what a block of code does or walk through logic.</td>
<td>Fast and accurate explanations.</td>
</tr>
<tr>
<th>Code commenting and documentation</th>
<td>Generate or refine comments and documentation.</td>
<td>Writes clear, concise explanations.</td>
</tr>
<tr>
<th>Bug investigation</th>
<td>Get a quick explanation or suggestion for an error.</td>
<td>Provides fast diagnostic insight.</td>
</tr>
<tr>
<th>Code snippet generation</th>
<td>Generate small, reusable pieces of code.</td>
<td>Delivers high-quality results quickly.</td>
</tr>
<tr>
<th>Multilingual prompts</th>
<td>Work with non-English prompts or identifiers.</td>
<td>Improved multilingual comprehension.</td>
</tr>
<tr>
<th>Image-based questions</th>
<td>Ask about a diagram or screenshot (where image input is supported).</td>
<td>Supports visual reasoning.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-1">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Multi-step reasoning or algorithms</th>
<td>Design complex logic or break down multi-step problems.</td>
<td>GPT-4.5 or Claude 3.7 Sonnet provide better step-by-step thinking.</td>
</tr>
<tr>
<th>Complex refactoring</th>
<td>Refactor large codebases or update multiple interdependent files.</td>
<td>GPT-4.5 handles context and code dependencies more robustly.</td>
</tr>
<tr>
<th>System review or architecture</th>
<td>Analyze structure, patterns, or architectural decisions in depth.</td>
<td>Claude 3.7 Sonnet or GPT-4.5 offer deeper analysis.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#gpt-45">GPT-4.5</a></h2>
<div>
<p>
GPT-4.5 in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>OpenAI GPT-4.5 improves reasoning, reliability, and contextual understanding. It works well for development tasks that involve complex logic, high-quality code generation, or interpreting nuanced intent.</p>
<p>Compared to GPT-4.1, GPT-4.5 produces more consistent results for multi-step reasoning, long-form content, and complex problem-solving. It may have slightly higher latency and costs than GPT-4.1 and other smaller models.</p>
<p>For more information about GPT-4.5, see <a href="https://platform.openai.com/docs/models/gpt-4.5-preview">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-2">Use cases</a></h3>
<p>GPT-4.5 is a good choice for tasks that involve multiple steps, require deeper code comprehension, or benefit from a conversational model that handles nuance well.</p>
<h3><a href="#strengths-2">Strengths</a></h3>
<p>The following table summarizes the strengths of GPT-4.5:</p>
<div>






























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why GPT-4.5 is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code documentation</th>
<td>Draft README files,  or technical explanations.</td>
<td>Generates clear, context-rich writing with minimal editing.</td>
</tr>
<tr>
<th>Complex code generation</th>
<td>Write full functions, classes, or multi-file logic.</td>
<td>Provides better structure, consistency, and fewer logic errors.</td>
</tr>
<tr>
<th>Bug investigation</th>
<td>Trace errors or walk through multi-step issues.</td>
<td>Maintains state and offers reliable reasoning across steps.</td>
</tr>
<tr>
<th>Decision-making prompts</th>
<td>Weigh pros and cons of libraries, patterns, or architecture.</td>
<td>Provides balanced, contextualized reasoning.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-2">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>




















<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>High-speed iteration</th>
<td>Rapid back-and-forth prompts or code tweaks.</td>
<td>GPT-4.1 responds faster with similar quality for lightweight tasks.</td>
</tr>
<tr>
<th>Cost-sensitive scenarios</th>
<td>Tasks where performance-to-cost ratio matters.</td>
<td>GPT-4.1 or o4-mini are more cost-effective.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#o1">o1</a></h2>
<p>OpenAI o1 is an older reasoning model that supports complex, multi-step tasks and deep logical reasoning to find the best solution.</p>
<p>For more information about o1, see <a href="https://platform.openai.com/docs/models/o1">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-3">Use cases</a></h3>
<p>o1 is a good choice for tasks that require deep logical reasoning. Its ability to reason through complex logic enables Copilot to break down problems into clear, actionable steps.
This makes o1 particularly well-suited for debugging. Its internal reasoning can extend beyond the original prompt to explore the broader context of a problem and can uncover edge cases or root causes that weren’t explicitly mentioned.</p>
<h3><a href="#strengths-3">Strengths</a></h3>
<p>The following table summarizes the strengths of o1:</p>
<div>



































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why o1 is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code optimization</th>
<td>Analyze and improve performance-critical or algorithmic code.</td>
<td>Excels at deep reasoning and identifying non-obvious improvements.</td>
</tr>
<tr>
<th>Debugging complex systems</th>
<td>Isolate and fix performance bottlenecks or multi-file issues.</td>
<td>Provides step-by-step analysis and high reasoning accuracy.</td>
</tr>
<tr>
<th>Structured code generation</th>
<td>Generate reusable functions, typed outputs, or structured  responses.</td>
<td>Supports function calling and structured output natively.</td>
</tr>
<tr>
<th>Analytical summarization</th>
<td>Interpret logs, benchmark results, or code behavior.</td>
<td>Translates raw data into clear, actionable insights.</td>
</tr>
<tr>
<th>Refactoring code</th>
<td>Improve maintainability and modularity of existing systems.</td>
<td>Applies deliberate and context-aware suggestions.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-3">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>




















<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Quick iterations</th>
<td>Rapid back-and-forth prompts or code tweaks.</td>
<td>GPT-4.1 or Gemini 2.0 Flash responds faster for lightweight tasks.</td>
</tr>
<tr>
<th>Cost-sensitive scenarios</th>
<td>Tasks where performance-to-cost ratio matters.</td>
<td>o4-mini or Gemini 2.0 Flash are more cost-effective for basic use cases.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#o3">o3</a></h2>
<div>
<p>
o3 in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>OpenAI o3 is the most capable reasoning model in the o-series. It is ideal for deep coding workflows and complex, multi-step tasks.
For more information about o3, see <a href="https://platform.openai.com/docs/models/o3">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-4">Use cases</a></h3>
<p>o3 is a good choice for tasks that require deep logical reasoning. Its ability to reason through complex logic enables Copilot to break down problems into clear, actionable steps.
This makes o3 particularly well-suited for debugging. Its internal reasoning can extend beyond the original prompt to explore the broader context of a problem and can uncover edge cases or root causes that weren’t explicitly mentioned.</p>
<h3><a href="#strengths-4">Strengths</a></h3>
<p>The following table summarizes the strengths of o3:</p>
<div>



































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why o3 is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code optimization</th>
<td>Analyze and improve performance-critical or algorithmic code.</td>
<td>Excels at deep reasoning and identifying non-obvious improvements.</td>
</tr>
<tr>
<th>Debugging complex systems</th>
<td>Isolate and fix performance bottlenecks or multi-file issues.</td>
<td>Provides step-by-step analysis and high reasoning accuracy.</td>
</tr>
<tr>
<th>Structured code generation</th>
<td>Generate reusable functions, typed outputs, or structured  responses.</td>
<td>Supports function calling and structured output natively.</td>
</tr>
<tr>
<th>Analytical summarization</th>
<td>Interpret logs, benchmark results, or code behavior.</td>
<td>Translates raw data into clear, actionable insights.</td>
</tr>
<tr>
<th>Refactoring code</th>
<td>Improve maintainability and modularity of existing systems.</td>
<td>Applies deliberate and context-aware suggestions.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-4">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>




















<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Quick iterations</th>
<td>Rapid back-and-forth prompts or code tweaks.</td>
<td>GPT-4.1 or Gemini 2.0 Flash responds faster for lightweight tasks.</td>
</tr>
<tr>
<th>Cost-sensitive scenarios</th>
<td>Tasks where performance-to-cost ratio matters.</td>
<td>o4-mini or Gemini 2.0 Flash are more cost-effective for basic use cases.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#o3-mini">o3-mini</a></h2>
<p>OpenAI o3-mini is a fast, cost-effective reasoning model designed to deliver coding performance while maintaining lower latency and resource usage. o3-mini outperforms o1 on coding benchmarks with response times that are comparable to o1-mini. Copilot is configured to use OpenAI's "medium" reasoning effort.</p>
<p>For more information about o3-mini, see <a href="https://platform.openai.com/docs/models/o3-mini">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-5">Use cases</a></h3>
<p>o3-mini is a good choice for developers who need fast, reliable answers to simple or repetitive coding questions. Its speed and efficiency make it ideal for lightweight development tasks.</p>
<h3><a href="#strengths-5">Strengths</a></h3>
<p>The following table summarizes the strengths of o3-mini:</p>
<div>






























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why o3-mini is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Real-time code suggestions</th>
<td>Write or extend basic functions and utilities.</td>
<td>Responds quickly with accurate, concise suggestions.</td>
</tr>
<tr>
<th>Code explanation</th>
<td>Understand what a block of code does or walk through logic.</td>
<td>Fast, accurate summaries with clear language.</td>
</tr>
<tr>
<th>Learn new concepts</th>
<td>Ask questions about programming concepts or patterns.</td>
<td>Offers helpful, accessible explanations with quick feedback.</td>
</tr>
<tr>
<th>Quick prototyping</th>
<td>Try out small ideas or test simple code logic quickly.</td>
<td>Fast, low-latency responses for iterative feedback.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-5">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Deep reasoning tasks</th>
<td>Multi-step analysis or architectural decisions.</td>
<td>GPT-4.5 or o1 provide more structured, thorough reasoning.</td>
</tr>
<tr>
<th>Creative or long-form tasks</th>
<td>Writing docs, refactoring across large codebases.</td>
<td>o3-mini is less expressive and structured than larger models.</td>
</tr>
<tr>
<th>Complex code generation</th>
<td>Write full functions, classes, or multi-file logic.</td>
<td>Larger models handle complexity and structure more reliably.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#o4-mini">o4-mini</a></h2>
<div>
<p>
o4-mini in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>OpenAI o4-mini is the most efficient model in the o-series. It is a cost-effective reasoning model designed to deliver coding performance while maintaining lower latency and resource usage.</p>
<p>For more information about o4, see <a href="https://platform.openai.com/docs/models/o4-mini">OpenAI's documentation</a>.</p>
<h3><a href="#use-cases-6">Use cases</a></h3>
<p>o4-mini is a good choice for developers who need fast, reliable answers to simple or repetitive coding questions. Its speed and efficiency make it ideal for lightweight development tasks.</p>
<h3><a href="#strengths-6">Strengths</a></h3>
<p>The following table summarizes the strengths of o4-mini:</p>
<div>






























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why o4-mini is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Real-time code suggestions</th>
<td>Write or extend basic functions and utilities.</td>
<td>Responds quickly with accurate, concise suggestions.</td>
</tr>
<tr>
<th>Code explanation</th>
<td>Understand what a block of code does or walk through logic.</td>
<td>Fast, accurate summaries with clear language.</td>
</tr>
<tr>
<th>Learn new concepts</th>
<td>Ask questions about programming concepts or patterns.</td>
<td>Offers helpful, accessible explanations with quick feedback.</td>
</tr>
<tr>
<th>Quick prototyping</th>
<td>Try out small ideas or test simple code logic quickly.</td>
<td>Fast, low-latency responses for iterative feedback.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-6">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Deep reasoning tasks</th>
<td>Multi-step analysis or architectural decisions.</td>
<td>GPT-4.5 or o3 provide more structured, thorough reasoning.</td>
</tr>
<tr>
<th>Creative or long-form tasks</th>
<td>Writing docs, refactoring across large codebases.</td>
<td>o4-mini is less expressive and structured than larger models.</td>
</tr>
<tr>
<th>Complex code generation</th>
<td>Write full functions, classes, or multi-file logic.</td>
<td>Larger models handle complexity and structure more reliably.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#claude-35-sonnet">Claude 3.5 Sonnet</a></h2>
<p>Claude 3.5 Sonnet is a fast and cost-efficient model designed for everyday developer tasks. While it doesn't have the deeper reasoning capabilities of Claude 3.7 Sonnet, it still performs well on coding tasks that require quick responses, clear summaries, and basic logic.</p>
<p>For more information about Claude 3.5 Sonnet, see <a href="https://www.anthropic.com/news/claude-3-5-sonnet">Anthropic's documentation</a>.
For more information on using Claude in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-claude-in-github-copilot">Using Claude in Copilot Chat</a>.</p>
<h3><a href="#use-cases-7">Use cases</a></h3>
<p>Claude 3.5 Sonnet is a good choice for everyday coding support—including writing documentation, answering language-specific questions, or generating boilerplate code. It offers helpful, direct answers without over-complicating the task.
If you're working within cost constraints, Claude 3.5 Sonnet is recommended as it delivers solid performance on many of the same tasks as Claude 3.7 Sonnet, but with significantly lower resource usage.</p>
<h3><a href="#strengths-7">Strengths</a></h3>
<p>The following table summarizes the strengths of Claude 3.5 Sonnet:</p>
<div>






























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why Claude 3.5 Sonnet is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code explanation</th>
<td>Understand what a block of code does or walk through logic.</td>
<td>Fast and accurate explanations.</td>
</tr>
<tr>
<th>Code commenting and documentation</th>
<td>Generate or refine comments and documentation.</td>
<td>Writes clear, concise explanations.</td>
</tr>
<tr>
<th>Quick language questions</th>
<td>Ask syntax, idiom, or feature-specific questions.</td>
<td>Offers fast and accurate explanations.</td>
</tr>
<tr>
<th>Code snippet generation</th>
<td>Generate small, reusable pieces of code.</td>
<td>Delivers high-quality results quickly.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-7">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Multi-step reasoning or algorithms</th>
<td>Design complex logic or break down multi-step problems.</td>
<td>GPT-4.5 or Claude 3.7 Sonnet provide better step-by-step thinking.</td>
</tr>
<tr>
<th>Complex refactoring</th>
<td>Refactor large codebases or update multiple interdependent files.</td>
<td>GPT-4.5 or Claude 3.7 Sonnet handle context and code dependencies more robustly.</td>
</tr>
<tr>
<th>System review or architecture</th>
<td>Analyze structure, patterns, or architectural decisions in depth.</td>
<td>Claude 3.7 Sonnet or GPT-4.5 offer deeper analysis.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#claude-37-sonnet">Claude 3.7 Sonnet</a></h2>
<p>Claude 3.7 Sonnet is a powerful model that excels in development tasks that require structured reasoning across large or complex codebases. Its hybrid approach to reasoning responds quickly when needed, while still supporting slower step-by-step analysis for deeper tasks.</p>
<p>For more information about Claude 3.7 Sonnet, see <a href="https://www.anthropic.com/claude/sonnet">Anthropic's documentation</a>.
For more information on using Claude in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-claude-in-github-copilot">Using Claude in Copilot Chat</a>.</p>
<h3><a href="#use-cases-8">Use cases</a></h3>
<p>Claude 3.7 Sonnet excels across the software development lifecycle, from initial design to bug fixes, maintenance to optimizations. It is particularly well-suited for multi-file refactoring or architectural planning, where understanding context across components is important.</p>
<h3><a href="#strengths-8">Strengths</a></h3>
<p>The following table summarizes the strengths of Claude 3.7 Sonnet:</p>
<div>



































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why Claude 3.7 Sonnet is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Multi-file refactoring</th>
<td>Improve structure and maintainability across large codebases.</td>
<td>Handles multi-step logic and retains cross-file context.</td>
</tr>
<tr>
<th>Architectural planning</th>
<td>Support mixed task complexity, from small queries to strategic work.</td>
<td>Fine-grained “thinking” controls adapt to the scope of each task.</td>
</tr>
<tr>
<th>Feature development</th>
<td>Build and implement functionality across frontend, backend, and API layers.</td>
<td>Supports tasks with structured reasoning and reliable completions.</td>
</tr>
<tr>
<th>Algorithm design</th>
<td>Design, test, and optimize complex algorithms.</td>
<td>Balances rapid prototyping with deep analysis when needed.</td>
</tr>
<tr>
<th>Analytical insights</th>
<td>Combine high-level summaries with deep dives into code behavior.</td>
<td>Hybrid reasoning lets the model shift based on user needs.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-8">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>

























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Quick iterations</th>
<td>Rapid back-and-forth prompts or code tweaks.</td>
<td>GPT-4.1 responds faster for lightweight tasks.</td>
</tr>
<tr>
<th>Cost-sensitive scenarios</th>
<td>Tasks where performance-to-cost ratio matters.</td>
<td>o4-mini or Gemini 2.0 Flash are more cost-effective for basic use cases. Claude 3.5 Sonnet is cheaper, simpler, and still advanced enough for similar tasks.</td>
</tr>
<tr>
<th>Lightweight prototyping</th>
<td>Rapid back-and-forth code iterations with minimal context.</td>
<td>Claude 3.7 Sonnet may over-engineer or apply unnecessary complexity.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#claude-sonnet-4">Claude Sonnet 4</a></h2>
<div>
<p>
Claude Sonnet 4 in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>For more information about Claude Sonnet 4, see <a href="https://www.anthropic.com/claude/">Anthropic's documentation</a>.
For more information on using Claude in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-claude-in-github-copilot">Using Claude in Copilot Chat</a>.</p>
<h2><a href="#claude-opus-4">Claude Opus 4</a></h2>
<div>
<p>
Claude Opus 4 in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>For more information about Claude Opus 4, see <a href="https://www.anthropic.com/claude/">Anthropic's documentation</a>.
For more information on using Claude in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-claude-in-github-copilot">Using Claude in Copilot Chat</a>.</p>
<h2><a href="#gemini-20-flash">Gemini 2.0 Flash</a></h2>
<p>Gemini 2.0 Flash is Google’s high-speed, multimodal model optimized for real-time, interactive applications that benefit from visual input and agentic reasoning. In Copilot Chat, Gemini 2.0 Flash enables fast responses and cross-modal understanding.</p>
<p>For more information about Gemini 2.0 Flash, see <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash">Google's documentation</a>.
For more information on using Gemini in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-gemini-in-github-copilot">Using Gemini in Copilot Chat</a>.</p>
<h3><a href="#use-cases-9">Use cases</a></h3>
<p>Gemini 2.0 Flash supports image input so that developers can bring visual context into tasks like UI inspection, diagram analysis, or layout debugging. This makes Gemini 2.0 Flash particularly useful for scenarios where image-based input enhances problem-solving, such as asking Copilot to analyze a UI screenshot for accessibility issues or to help understand a visual bug in a layout.</p>
<h3><a href="#strengths-9">Strengths</a></h3>
<p>The following table summarizes the strengths of Gemini 2.0 Flash:</p>
<div>



































<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why Gemini 2.0 Flash is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Code snippet generation</th>
<td>Generate small, reusable pieces of code.</td>
<td>Delivers high-quality results quickly.</td>
</tr>
<tr>
<th>Design feedback loops</th>
<td>Get suggestions from sketches, diagrams, or visual drafts</td>
<td>Supports visual reasoning.</td>
</tr>
<tr>
<th>Image-based analysis</th>
<td>Ask about a diagram or screenshot (where image input is supported).</td>
<td>Supports visual reasoning.</td>
</tr>
<tr>
<th>Front-end prototyping</th>
<td>Build and test UIs or workflows involving visual elements</td>
<td>Supports multimodal reasoning and lightweight context.</td>
</tr>
<tr>
<th>Bug investigation</th>
<td>Get a quick explanation or suggestion for an error.</td>
<td>Provides fast diagnostic insight.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-9">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>




















<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody>
<tr>
<th>Multi-step reasoning or algorithms</th>
<td>Design complex logic or break down multi-step problems.</td>
<td>GPT-4.5 or Claude 3.7 Sonnet provide better step-by-step thinking.</td>
</tr>
<tr>
<th>Complex refactoring</th>
<td>Refactor large codebases or update multiple interdependent files.</td>
<td>GPT-4.5 handles context and code dependencies more robustly.</td>
</tr>
</tbody>
</table>
</div>
<h2><a href="#gemini-25-pro">Gemini 2.5 Pro</a></h2>
<div>
<p>
Gemini 2.5 Pro in Copilot Chat is currently in public preview and subject to change.</p>
</div>
<p>Gemini 2.5 Pro is Google's latest AI model, designed to handle complex tasks with advanced reasoning and coding capabilities. It also works well for heavy research workflows that require long-context understanding and analysis.</p>
<p>For more information about Gemini 2.5 Pro, see <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro">Google's documentation</a>.
For more information on using Gemini in Copilot, see <a href="/en/copilot/using-github-copilot/ai-models/using-gemini-in-github-copilot">Using Gemini in Copilot Chat</a>.</p>
<h3><a href="#use-cases-10">Use cases</a></h3>
<p>Gemini 2.5 Pro is well-suited for advanced coding tasks, such as developing complex algorithms or debugging intricate codebases.
It can assist with scientific research by analyzing data and generating insights across a wide range of disciplines. Its long-context capabilities allow it to manage and understand extensive documents or datasets effectively.
Gemini 2.5 Pro is a strong choice for developers needing a powerful model.</p>
<h3><a href="#strengths-10">Strengths</a></h3>
<p>The following table summarizes the strengths of Gemini 2.5 Pro:</p>
<div>






























<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why Gemini 2.5 Pro is a good fit</th>
</tr></thead>
<tbody>
<tr>
<th>Complex code generation</th>
<td>Write full functions, classes, or multi-file logic.</td>
<td>Provides better structure, consistency, and fewer logic errors.</td>
</tr>
<tr>
<th>Debugging complex systems</th>
<td>Isolate and fix performance bottlenecks or multi-file issues.</td>
<td>Provides step-by-step analysis and high reasoning accuracy.</td>
</tr>
<tr>
<th>Scientific research</th>
<td>Analyze data and generate insights across scientific disciplines.</td>
<td>Supports complex analysis with heavy researching capabilities.</td>
</tr>
<tr>
<th>Long-context processing</th>
<td>Analyze extensive documents, datasets, or codebases.</td>
<td>Handles long-context inputs effectively.</td>
</tr>
</tbody>
</table>
</div>
<h3><a href="#alternative-options-10">Alternative options</a></h3>
<p>The following table summarizes when an alternative model may be a better choice:</p>
<div>















<table>
<thead><tr>
<th>Task</th>
<th>Description</th>
<th>Why another model may be better</th>
</tr></thead>
<tbody><tr>
<th>Cost-sensitive scenarios</th>
<td>Tasks where performance-to-cost ratio matters.</td>
<td>o4-mini or Gemini 2.0 Flash are more cost-effective for basic use cases.</td>
</tr></tbody>
</table>
</div>
<h2><a href="#further-reading">Further reading</a></h2>
<ul>
<li><a href="/en/copilot/using-github-copilot/ai-models/examples-for-ai-model-comparison">Comparing AI models using different tasks</a></li>
<li><a href="/en/copilot/using-github-copilot/ai-models/changing-the-ai-model-for-copilot-chat">Changing the AI model for Copilot Chat</a></li>
<li><a href="/en/copilot/using-github-copilot/ai-models/changing-the-ai-model-for-copilot-code-completion">Changing the AI model for Copilot code completion</a></li>
</ul>
</div></div></div>
</div>
</div></main>
</div></body></html>